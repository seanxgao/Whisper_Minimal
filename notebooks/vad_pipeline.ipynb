{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAD Training Pipeline\n",
        "\n",
        "This notebook provides a complete end-to-end workflow for training a tiny VAD model using knowledge distillation.\n",
        "\n",
        "## Workflow Steps\n",
        "\n",
        "1. **Imports & Config** - Load dependencies and configuration\n",
        "2. **Dataset Discovery & Split** - Scan WAV directory and create train/val/test splits\n",
        "3. **Preprocessing** - Extract features, generate teacher labels, create chunks\n",
        "4. **Quick Data Inspection** - Visualize sample chunks and metadata\n",
        "5. **Training** - Train the tiny VAD model on chunks\n",
        "6. **Evaluate a Sample WAV** - Test the trained model on a sample audio file\n",
        "7. **Export ONNX** - Export model to ONNX format for deployment\n",
        "8. **Final Summary** - Display training results and file locations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Imports & Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: h:\\Personal Projects\\VAD_Training\n",
            "Working directory: h:\\Personal Projects\\VAD_Training\\notebooks\n",
            "Configuration loaded:\n",
            "{\n",
            "  \"chunk_size\": 100,\n",
            "  \"stride\": 50,\n",
            "  \"n_mels\": 80,\n",
            "  \"chunks_dir\": \"data/chunks\",\n",
            "  \"teacher_labels_dir\": \"data/teacher_labels\",\n",
            "  \"data\": {\n",
            "    \"sample_list\": \"data/sample_list.txt\",\n",
            "    \"num_samples\": 2000\n",
            "  },\n",
            "  \"batch_size\": 32,\n",
            "  \"learning_rate\": 0.001,\n",
            "  \"epochs\": 100,\n",
            "  \"num_workers\": 4,\n",
            "  \"random_seed\": 2025,\n",
            "  \"use_tensorboard\": true,\n",
            "  \"save_every_n_steps\": 1000,\n",
            "  \"validate_every_n_epochs\": 5,\n",
            "  \"resume_from\": null,\n",
            "  \"model\": {\n",
            "    \"n_mels\": 80,\n",
            "    \"hidden_dims\": [\n",
            "      32,\n",
            "      64,\n",
            "      32\n",
            "    ],\n",
            "    \"kernel_sizes\": [\n",
            "      3,\n",
            "      3,\n",
            "      3\n",
            "    ],\n",
            "    \"dropout\": 0.1\n",
            "  },\n",
            "  \"paths\": {\n",
            "    \"checkpoint_dir\": \"vad_distill/distill/tiny_vad/checkpoints\",\n",
            "    \"onnx_dir\": \"models/tiny_vad/onnx\",\n",
            "    \"logs_dir\": \"logs\"\n",
            "  }\n",
            "}\n",
            "\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA GeForce RTX 3080 Ti\n",
            "CUDA version: 11.8\n"
          ]
        }
      ],
      "source": [
        "# Setup Python path to include project root\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Get project root directory (parent of notebooks directory)\n",
        "notebook_dir = Path.cwd()\n",
        "if notebook_dir.name == 'notebooks':\n",
        "    project_root = notebook_dir.parent\n",
        "else:\n",
        "    # If running from project root, use current directory\n",
        "    project_root = notebook_dir\n",
        "\n",
        "# Add project root to Python path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Working directory: {Path.cwd()}\")\n",
        "\n",
        "# Standard library imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Import preprocessing pipeline (this handles sampling logic)\n",
        "from vad_distill.scripts.run_preprocessing_pipeline import run_preprocessing_pipeline\n",
        "from preprocessing.chunk_config import CHUNK_SIZE, N_MELS, SAMPLE_RATE\n",
        "\n",
        "# Import dataset and model\n",
        "from vad_distill.distill.tiny_vad.dataset import TinyVADChunkDataset\n",
        "from vad_distill.distill.tiny_vad.train import train_tiny_vad\n",
        "from vad_distill.distill.tiny_vad.model import TinyVADModel, build_tiny_vad_model\n",
        "from vad_distill.distill.tiny_vad.export_onnx import export_tiny_vad_onnx\n",
        "\n",
        "# Import test function\n",
        "from vad_distill.scripts.test_single_wav import test_single_wav\n",
        "\n",
        "# Import config loader\n",
        "from vad_distill.utils.config import load_yaml\n",
        "\n",
        "# Load configuration (use absolute path based on project_root)\n",
        "config_path = project_root / \"vad_distill\" / \"configs\" / \"student_tiny_vad.yaml\"\n",
        "config = load_yaml(str(config_path))\n",
        "print(\"Configuration loaded:\")\n",
        "print(json.dumps(config, indent=2))\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Dataset Discovery & Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 37325 audio files\n",
            "Estimated total duration: 12694238.80 seconds (3526.18 hours)\n",
            "Average file duration: 340.10 seconds\n",
            "\n",
            "Split: Train=26127, Val=5598, Test=5600\n",
            "\n",
            "Manifest file created: h:\\Personal Projects\\VAD_Training\\data\\splits\\train_manifest.jsonl\n",
            "Total files in manifest: 26127\n",
            "Note: Preprocessing will apply sampling logic from config\n",
            "  - sample_list: data/sample_list.txt\n",
            "  - num_samples: 2000\n",
            "\n",
            "Sample train files:\n",
            "  X0000027864_330958324.opus\n",
            "  X0000003112_4617142.opus\n",
            "  X0000027920_331675656.opus\n"
          ]
        }
      ],
      "source": [
        "# User specifies audio directory path\n",
        "# For WenetSpeech dataset, use: \"H:/wenet_data/audio/train/podcast\" or similar\n",
        "audio_dir = Path(\"H:/wenet_data/audio/train/podcast\")  # CHANGE THIS to your audio directory\n",
        "\n",
        "# Recursively scan for audio files (WAV and OPUS)\n",
        "# WenetSpeech uses OPUS format, but we support both\n",
        "audio_files = []\n",
        "if audio_dir.exists():\n",
        "    # Recursive search for audio files\n",
        "    audio_files.extend(audio_dir.rglob(\"*.wav\"))\n",
        "    audio_files.extend(audio_dir.rglob(\"*.WAV\"))\n",
        "    audio_files.extend(audio_dir.rglob(\"*.opus\"))\n",
        "    audio_files.extend(audio_dir.rglob(\"*.OPUS\"))\n",
        "    audio_files = sorted(list(set(audio_files)))  # Remove duplicates and sort\n",
        "else:\n",
        "    print(f\"Warning: Directory not found: {audio_dir}\")\n",
        "    print(\"Please update audio_dir to point to your audio files directory\")\n",
        "\n",
        "print(f\"Found {len(audio_files)} audio files\")\n",
        "\n",
        "# Calculate total duration (approximate)\n",
        "if len(audio_files) > 0:\n",
        "    from vad_distill.utils.audio_io import load_wav\n",
        "    total_duration = 0.0\n",
        "    sample_size = min(10, len(audio_files))\n",
        "    successful_samples = 0\n",
        "    \n",
        "    for audio_file in audio_files[:sample_size]:\n",
        "        try:\n",
        "            waveform = load_wav(str(audio_file), target_sr=SAMPLE_RATE)\n",
        "            total_duration += len(waveform) / SAMPLE_RATE\n",
        "            successful_samples += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load {audio_file.name}: {e}\")\n",
        "            pass\n",
        "    \n",
        "    if successful_samples > 0:\n",
        "        avg_duration = total_duration / successful_samples\n",
        "        estimated_total = avg_duration * len(audio_files)\n",
        "        print(f\"Estimated total duration: {estimated_total:.2f} seconds ({estimated_total/3600:.2f} hours)\")\n",
        "        print(f\"Average file duration: {avg_duration:.2f} seconds\")\n",
        "    else:\n",
        "        print(\"Warning: Could not load any sample files for duration estimation\")\n",
        "else:\n",
        "    print(\"No audio files found. Please check the directory path.\")\n",
        "\n",
        "# Check if we have files before splitting\n",
        "if len(audio_files) == 0:\n",
        "    print(\"\\nERROR: No audio files found. Cannot proceed with dataset split.\")\n",
        "    print(\"Please check the audio_dir path and ensure it contains audio files.\")\n",
        "    train_files = []\n",
        "    val_files = []\n",
        "    test_files = []\n",
        "else:\n",
        "    # Shuffle with fixed seed\n",
        "    seed = config.get('random_seed', 2025)\n",
        "    random.seed(seed)\n",
        "    audio_files_shuffled = audio_files.copy()\n",
        "    random.shuffle(audio_files_shuffled)\n",
        "    \n",
        "    # Split into train/val/test (70/15/15)\n",
        "    num_total = len(audio_files_shuffled)\n",
        "    num_train = int(0.7 * num_total)\n",
        "    num_val = int(0.15 * num_total)\n",
        "    num_test = num_total - num_train - num_val\n",
        "    \n",
        "    train_files = audio_files_shuffled[:num_train]\n",
        "    val_files = audio_files_shuffled[num_train:num_train+num_val]\n",
        "    test_files = audio_files_shuffled[num_train+num_val:]\n",
        "    \n",
        "    print(f\"\\nSplit: Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}\")\n",
        "\n",
        "# Create manifest file for preprocessing pipeline (JSONL format)\n",
        "# The preprocessing pipeline will apply sampling logic based on config\n",
        "if len(train_files) > 0:\n",
        "    splits_dir = project_root / \"data\" / \"splits\"\n",
        "    splits_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Create manifest file for training (JSONL format)\n",
        "    manifest_path = splits_dir / \"train_manifest.jsonl\"\n",
        "    with open(manifest_path, 'w', encoding='utf-8') as f:\n",
        "        for audio_file in train_files:\n",
        "            item = {\n",
        "                \"utt_id\": audio_file.stem,\n",
        "                \"wav_path\": str(audio_file.resolve())\n",
        "            }\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "    \n",
        "    # Also save split lists for reference (val and test)\n",
        "    with open(splits_dir / \"val_list.txt\", \"w\") as f:\n",
        "        for audio_file in val_files:\n",
        "            f.write(f\"{audio_file}\\n\")\n",
        "    \n",
        "    with open(splits_dir / \"test_list.txt\", \"w\") as f:\n",
        "        for audio_file in test_files:\n",
        "            f.write(f\"{audio_file}\\n\")\n",
        "    \n",
        "    print(f\"\\nManifest file created: {manifest_path}\")\n",
        "    print(f\"Total files in manifest: {len(train_files)}\")\n",
        "    print(f\"Note: Preprocessing will apply sampling logic from config\")\n",
        "    print(f\"  - sample_list: {config.get('data', {}).get('sample_list', 'not configured')}\")\n",
        "    print(f\"  - num_samples: {config.get('data', {}).get('num_samples', 'not configured')}\")\n",
        "    print(\"\\nSample train files:\")\n",
        "    for audio_file in train_files[:3]:\n",
        "        print(f\"  {audio_file.name}\")\n",
        "else:\n",
        "    print(\"\\nNo manifest file created (no files found)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using manifest file: h:\\Personal Projects\\VAD_Training\\data\\splits\\train_manifest.jsonl\n",
            "Configuration:\n",
            "  - sample_list: data/sample_list.txt\n",
            "  - num_samples: 2000\n",
            "\n",
            "Loaded fixed sample list from: H:\\Personal Projects\\VAD_Training\\data\\sample_list.txt\n",
            "Using fixed subset: 2000 files\n",
            "Notice: ffmpeg is not installed. torchaudio is used to load audio\n",
            "If you want to use ffmpeg backend to load audio, please install it by:\n",
            "\tsudo apt install ffmpeg # ubuntu\n",
            "\t# brew install ffmpeg # mac\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing audio files:   6%|â–Œ         | 111/2000 [05:05<1:36:26,  3.06s/it]"
          ]
        }
      ],
      "source": [
        "# Run preprocessing pipeline using run_preprocessing_pipeline\n",
        "# This will automatically apply deterministic sampling based on config.data.sample_list\n",
        "# The pipeline will only process files from the sampled list\n",
        "\n",
        "import time\n",
        "\n",
        "# Setup paths\n",
        "output_root = project_root / \"data\"\n",
        "teacher_model_dir = project_root / \"teacher\"\n",
        "manifest_path = project_root / \"data\" / \"splits\" / \"train_manifest.jsonl\"\n",
        "\n",
        "# Check if manifest exists\n",
        "if not manifest_path.exists():\n",
        "    print(f\"ERROR: Manifest file not found: {manifest_path}\")\n",
        "    print(\"Please run Cell 4 first to create the manifest file.\")\n",
        "else:\n",
        "    print(f\"Using manifest file: {manifest_path}\")\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"  - sample_list: {config.get('data', {}).get('sample_list', 'not configured')}\")\n",
        "    print(f\"  - num_samples: {config.get('data', {}).get('num_samples', 'not configured')}\")\n",
        "    print()\n",
        "    \n",
        "    # Run preprocessing pipeline\n",
        "    # This will automatically:\n",
        "    # 1. Load all WAV files from manifest\n",
        "    # 2. Apply deterministic sampling (if configured)\n",
        "    # 3. Process only the sampled files\n",
        "    start_time = time.time()\n",
        "    \n",
        "    stats = run_preprocessing_pipeline(\n",
        "        manifest_path=str(manifest_path),\n",
        "        output_root=str(output_root),\n",
        "        teacher_model_dir=str(teacher_model_dir),\n",
        "        device=\"cpu\",\n",
        "        config=config,\n",
        "    )\n",
        "    \n",
        "    # Calculate total time\n",
        "    total_time = time.time() - start_time\n",
        "    hours = int(total_time // 3600)\n",
        "    minutes = int((total_time % 3600) // 60)\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\nPreprocessing complete!\")\n",
        "    print(f\"Files processed: {stats['success_count']}\")\n",
        "    print(f\"Total chunks: {stats['total_chunks']:,}\")\n",
        "    if hours > 0:\n",
        "        print(f\"Total time: {hours}h {minutes}m\")\n",
        "    else:\n",
        "        print(f\"Total time: {minutes}m\")\n",
        "    print(f\"Chunks directory: {stats['chunks_dir']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample chunk (use project_root for absolute paths)\n",
        "chunks_dir = project_root / \"data\" / \"chunks\"\n",
        "chunk_files = sorted(chunks_dir.glob(\"chunk_*.npy\"))\n",
        "if len(chunk_files) > 0:\n",
        "    sample_chunk_path = chunk_files[0]\n",
        "    chunk_data = np.load(sample_chunk_path, allow_pickle=True).item()\n",
        "    \n",
        "    features = chunk_data['features']  # (100, 80)\n",
        "    labels = chunk_data['labels']      # (100,)\n",
        "    \n",
        "    print(f\"Chunk shape: features={features.shape}, labels={labels.shape}\")\n",
        "    print(f\"Chunk UID: {chunk_data.get('uid', 'unknown')}\")\n",
        "    \n",
        "    # Visualize spectrogram\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.imshow(features.T, aspect='auto', origin='lower', cmap='viridis')\n",
        "    plt.colorbar(label='Log-mel energy')\n",
        "    plt.title('Log-mel Spectrogram')\n",
        "    plt.xlabel('Frame index')\n",
        "    plt.ylabel('Mel bin')\n",
        "    \n",
        "    # Overlay labels\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(labels, 'r-', linewidth=2, label='Teacher VAD probability')\n",
        "    plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='Threshold')\n",
        "    plt.xlabel('Frame index')\n",
        "    plt.ylabel('VAD probability')\n",
        "    plt.title('Teacher VAD Labels')\n",
        "    plt.legend()\n",
        "    plt.ylim([0, 1])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print metadata\n",
        "    metadata_path = chunks_dir / \"metadata.json\"\n",
        "    if metadata_path.exists():\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(\"\\nMetadata.json content:\")\n",
        "        print(json.dumps(metadata, indent=2))\n",
        "else:\n",
        "    print(\"No chunks found. Run Cell 3 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model using existing train_tiny_vad function\n",
        "# This function handles dataset creation, splitting, and training loop internally\n",
        "train_tiny_vad(config)\n",
        "\n",
        "# Load training history for plotting (use project_root for absolute paths)\n",
        "checkpoint_dir_rel = config.get('paths', {}).get('checkpoint_dir', 'vad_distill/distill/tiny_vad/checkpoints')\n",
        "checkpoint_dir = project_root / checkpoint_dir_rel if not Path(checkpoint_dir_rel).is_absolute() else Path(checkpoint_dir_rel)\n",
        "history_path = checkpoint_dir / \"train_history.json\"\n",
        "\n",
        "if history_path.exists():\n",
        "    with open(history_path, 'r') as f:\n",
        "        history = json.load(f)\n",
        "    \n",
        "    # Extract training curves\n",
        "    epochs = [h['epoch'] for h in history]\n",
        "    train_losses = [h['train_loss'] for h in history]\n",
        "    val_losses = [h['val_loss'] for h in history if h['val_loss'] is not None]\n",
        "    val_epochs = [h['epoch'] for h in history if h['val_loss'] is not None]\n",
        "    \n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "    if len(val_losses) > 0:\n",
        "        plt.plot(val_epochs, val_losses, 'r-', label='Val Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Curves')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nTraining complete!\")\n",
        "    print(f\"Best checkpoint: {checkpoint_dir / 'best.pt'}\")\n",
        "    print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
        "    if len(val_losses) > 0:\n",
        "        print(f\"Best val loss: {min(val_losses):.6f}\")\n",
        "else:\n",
        "    print(\"Training history not found. Check logs for details.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Evaluate a Sample WAV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a sample WAV file for evaluation\n",
        "sample_wav = test_files[0] if len(test_files) > 0 else train_files[0]\n",
        "print(f\"Evaluating: {sample_wav}\")\n",
        "\n",
        "# Load model checkpoint (use project_root for absolute paths)\n",
        "# Ensure checkpoint_dir is defined (from Cell 5 or define here)\n",
        "if 'checkpoint_dir' not in globals():\n",
        "    checkpoint_dir_rel = config.get('paths', {}).get('checkpoint_dir', 'vad_distill/distill/tiny_vad/checkpoints')\n",
        "    checkpoint_dir = project_root / checkpoint_dir_rel if not Path(checkpoint_dir_rel).is_absolute() else Path(checkpoint_dir_rel)\n",
        "checkpoint_path = checkpoint_dir / \"best.pt\"\n",
        "config_path = str(project_root / \"vad_distill\" / \"configs\" / \"student_tiny_vad.yaml\")\n",
        "output_dir = project_root / \"outputs\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Run inference using existing test function\n",
        "test_single_wav(\n",
        "    wav_path=sample_wav,\n",
        "    model_path=checkpoint_path,\n",
        "    output_dir=output_dir,\n",
        "    config_path=config_path,\n",
        "    threshold=0.5,\n",
        "    use_onnx=False,\n",
        ")\n",
        "\n",
        "# Load and visualize results\n",
        "wav_name = sample_wav.stem\n",
        "scores_path = output_dir / f\"{wav_name}_scores.npy\"\n",
        "segments_path = output_dir / f\"{wav_name}_segments.json\"\n",
        "\n",
        "if scores_path.exists():\n",
        "    scores = np.load(scores_path)\n",
        "    \n",
        "    # Load audio for waveform display\n",
        "    from vad_distill.utils.audio_io import load_wav\n",
        "    wav = load_wav(str(sample_wav), target_sr=SAMPLE_RATE)\n",
        "    time_axis = np.arange(len(wav)) / SAMPLE_RATE\n",
        "    frame_time = np.arange(len(scores)) * 0.01  # 10ms per frame\n",
        "    \n",
        "    # Plot results\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    \n",
        "    # Waveform\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(time_axis, wav, 'b-', linewidth=0.5)\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title('Waveform')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # VAD scores\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(frame_time, scores, 'r-', linewidth=1.5, label='VAD probability')\n",
        "    plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='Threshold')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('VAD probability')\n",
        "    plt.title('Model VAD Output (Postprocessed)')\n",
        "    plt.legend()\n",
        "    plt.ylim([0, 1])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Segments overlay\n",
        "    if segments_path.exists():\n",
        "        with open(segments_path, 'r') as f:\n",
        "            segments = json.load(f)\n",
        "        plt.subplot(3, 1, 3)\n",
        "        for start, end in segments:\n",
        "            plt.axvspan(start, end, alpha=0.3, color='green')\n",
        "        plt.plot(frame_time, scores, 'r-', linewidth=1.5)\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('VAD probability')\n",
        "        plt.title('Speech Segments (Green)')\n",
        "        plt.ylim([0, 1])\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nResults saved to {output_dir}\")\n",
        "else:\n",
        "    print(\"Inference output not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Export ONNX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export model to ONNX format\n",
        "export_tiny_vad_onnx(config, checkpoint_path=None)\n",
        "\n",
        "# Verify ONNX file exists (use project_root for absolute paths)\n",
        "onnx_dir_rel = config.get('paths', {}).get('onnx_dir', 'models/tiny_vad/onnx')\n",
        "onnx_dir = project_root / onnx_dir_rel if not Path(onnx_dir_rel).is_absolute() else Path(onnx_dir_rel)\n",
        "onnx_path = onnx_dir / \"tiny_vad.onnx\"\n",
        "\n",
        "if onnx_path.exists():\n",
        "    print(f\"ONNX export OK: {onnx_path}\")\n",
        "    print(f\"File size: {onnx_path.stat().st_size / 1024:.2f} KB\")\n",
        "    \n",
        "    # Optional: Test ONNX inference\n",
        "    try:\n",
        "        import onnxruntime as ort\n",
        "        import numpy as np\n",
        "        \n",
        "        session = ort.InferenceSession(str(onnx_path))\n",
        "        dummy_input = np.random.randn(1, 100, 80).astype(np.float32)\n",
        "        outputs = session.run(None, {'mel_features': dummy_input})\n",
        "        print(f\"ONNX inference test passed. Output shape: {outputs[0].shape}\")\n",
        "    except ImportError:\n",
        "        print(\"onnxruntime not available, skipping ONNX inference test\")\n",
        "    except Exception as e:\n",
        "        print(f\"ONNX inference test failed: {e}\")\n",
        "else:\n",
        "    print(f\"ONNX export failed. Check logs for details.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Final Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print final summary\n",
        "print(\"=\" * 60)\n",
        "print(\"VAD TRAINING PIPELINE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensure checkpoint_dir and onnx_dir are defined\n",
        "if 'checkpoint_dir' not in globals():\n",
        "    checkpoint_dir_rel = config.get('paths', {}).get('checkpoint_dir', 'vad_distill/distill/tiny_vad/checkpoints')\n",
        "    checkpoint_dir = project_root / checkpoint_dir_rel if not Path(checkpoint_dir_rel).is_absolute() else Path(checkpoint_dir_rel)\n",
        "if 'onnx_dir' not in globals():\n",
        "    onnx_dir_rel = config.get('paths', {}).get('onnx_dir', 'models/tiny_vad/onnx')\n",
        "    onnx_dir = project_root / onnx_dir_rel if not Path(onnx_dir_rel).is_absolute() else Path(onnx_dir_rel)\n",
        "if 'config_path' not in globals():\n",
        "    config_path = str(project_root / \"vad_distill\" / \"configs\" / \"student_tiny_vad.yaml\")\n",
        "\n",
        "# Checkpoint location\n",
        "checkpoint_path = checkpoint_dir / \"best.pt\"\n",
        "print(f\"\\nCheckpoint saved: {checkpoint_path}\")\n",
        "print(f\"  Exists: {checkpoint_path.exists()}\")\n",
        "\n",
        "# Chunks location (use project_root for absolute paths)\n",
        "chunks_dir = project_root / \"data\" / \"chunks\"\n",
        "chunk_files = list(chunks_dir.glob(\"chunk_*.npy\"))\n",
        "print(f\"\\nChunks directory: {chunks_dir}\")\n",
        "print(f\"  Total chunks: {len(chunk_files)}\")\n",
        "print(f\"  Metadata: {chunks_dir / 'metadata.json'}\")\n",
        "\n",
        "# Config used\n",
        "print(f\"\\nConfig used: {config_path}\")\n",
        "print(f\"  Batch size: {config.get('batch_size', 'N/A')}\")\n",
        "print(f\"  Learning rate: {config.get('learning_rate', 'N/A')}\")\n",
        "print(f\"  Epochs: {config.get('epochs', 'N/A')}\")\n",
        "\n",
        "# ONNX file\n",
        "onnx_path = onnx_dir / \"tiny_vad.onnx\"\n",
        "print(f\"\\nONNX file: {onnx_path}\")\n",
        "print(f\"  Exists: {onnx_path.exists()}\")\n",
        "\n",
        "# Training history\n",
        "if history_path.exists():\n",
        "    with open(history_path, 'r') as f:\n",
        "        history = json.load(f)\n",
        "    final_epoch = history[-1]['epoch']\n",
        "    final_train_loss = history[-1]['train_loss']\n",
        "    val_losses = [h['val_loss'] for h in history if h['val_loss'] is not None]\n",
        "    print(f\"\\nTraining completed:\")\n",
        "    print(f\"  Final epoch: {final_epoch}\")\n",
        "    print(f\"  Final train loss: {final_train_loss:.6f}\")\n",
        "    if len(val_losses) > 0:\n",
        "        print(f\"  Best val loss: {min(val_losses):.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vad_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
