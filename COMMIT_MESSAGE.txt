Initial commit: VAD distillation project setup

This is a personal ML experiment to explore knowledge distillation for Voice Activity Detection (VAD). The project implements a teacher-student architecture where a pretrained FSMN-VAD teacher model generates labels for training two lightweight student models.

What's included:
- Teacher model wrapper for FSMN-VAD (FunASR)
- Student A: Tiny frame-level VAD model (1D CNN)
- Student B: Chunk trigger model for boundary detection
- Complete training pipeline with PyTorch
- ONNX export functionality for deployment
- Configuration-driven design with YAML configs
- Modular code structure with clear separation of concerns

The codebase is structured as a Python package with utilities for audio processing, feature extraction, and model training. All paths are configurable and the project follows a clean, modular architecture.

Note: This is an experimental project for learning and exploration, not production-ready code. Some TODOs remain for future refinement (e.g., FunASR output parsing, soft-label distillation).

